{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: Babaniyi Olaniyi (horlaneyee@gmail.com)\n",
    "### Data description: DatasetDaily Week 2 submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART A: Descriptive Statistics, Visualisation and Insight\n",
    "1. How does consumers behaviours change with economic factors?\n",
    "2. How is the manufacturing sector affected by the probability of recession?\n",
    "3. How is the financial sector by government policies?\n",
    "4. How is the government affected by a downturn in the economy?\n",
    "5. How do we know when an economy is booming?\n",
    "\n",
    "## PART B: Modelling and Estimation\n",
    "\n",
    "Compare different models for recession prediction, focusing on methods from the field of Machine Learning (ML).\n",
    "\n",
    "Previous attempts of predicting recession were generally hampered by data availability as evident in this study as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Index(['observation_date', 'total_employment', 'consumer_credit',\n",
      "       'credit_delinquency_rate', 'unemployment_rate', 'federal_funds_rate',\n",
      "       'consumer_opinion', 'recession_probability',\n",
      "       'manufacturers_new_orders_durable_goods',\n",
      "       'manufacturer_new_orders_consumer_goods', 'total_vehicle_sales',\n",
      "       'industrial_production_index', 'new_one_family_houses_sold', 'year',\n",
      "       'month'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "___ Descriptive stats of the probability of recession____\n",
      "count    634.000000\n",
      "mean      10.549385\n",
      "std       26.724375\n",
      "min        0.000000\n",
      "25%        0.020000\n",
      "50%        0.080000\n",
      "75%        0.620000\n",
      "max      100.000000\n",
      "Name: recession_probability, dtype: float64\n",
      "\n",
      "____Categorising probabilities of recession_____\n",
      "\n",
      "____Some fun with where____\n",
      "\n",
      "0    914\n",
      "1     62\n",
      "Name: recession, dtype: int64\n",
      "\n",
      "____Number of missing values by column____\n",
      "manufacturer_new_orders_consumer_goods    639\n",
      "manufacturers_new_orders_durable_goods    637\n",
      "credit_delinquency_rate                   628\n",
      "total_vehicle_sales                       445\n",
      "recession_probability                     342\n",
      "new_one_family_houses_sold                288\n",
      "consumer_opinion                          253\n",
      "federal_funds_rate                        186\n",
      "unemployment_rate                         108\n",
      "consumer_credit                            50\n",
      "total_employment                            1\n",
      "recession                                   0\n",
      "month                                       0\n",
      "year                                        0\n",
      "industrial_production_index                 0\n",
      "observation_date                            0\n",
      "dtype: int64\n",
      "\n",
      "____Number of missing values total: 3577____\n",
      "\n",
      "____Dropping missing values____\n",
      "\n",
      "______Data types____\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 335 entries, 0 to 334\n",
      "Data columns (total 16 columns):\n",
      "observation_date                          335 non-null datetime64[ns]\n",
      "total_employment                          335 non-null float64\n",
      "consumer_credit                           335 non-null float64\n",
      "credit_delinquency_rate                   335 non-null float64\n",
      "unemployment_rate                         335 non-null float64\n",
      "federal_funds_rate                        335 non-null float64\n",
      "consumer_opinion                          335 non-null float64\n",
      "recession_probability                     335 non-null float64\n",
      "manufacturers_new_orders_durable_goods    335 non-null object\n",
      "manufacturer_new_orders_consumer_goods    335 non-null float64\n",
      "total_vehicle_sales                       335 non-null float64\n",
      "industrial_production_index               335 non-null object\n",
      "new_one_family_houses_sold                335 non-null object\n",
      "year                                      335 non-null category\n",
      "month                                     335 non-null category\n",
      "recession                                 335 non-null int64\n",
      "dtypes: category(2), datetime64[ns](1), float64(9), int64(1), object(3)\n",
      "memory usage: 39.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning and Wrangling\n",
    "\n",
    "df = pd.read_excel(\"/eos/home-f/franzoni/SWAN_projects/DatasetDaily/Week2_COVID_Economics/data/economics.xlsx\")\n",
    "df['observation_date'] = pd.to_datetime(df['observation_date'])\n",
    "df['year'] = df['observation_date'].dt.year\n",
    "df['month'] = df['observation_date'].dt.month\n",
    "\n",
    "# GF fixes hames of culums where units have been added after the notebook was written... :-(\n",
    "# remove units\n",
    "df = ( df.rename(columns={'unemployment_rate (%)': 'unemployment_rate',\n",
    "                        'credit_delinquency_rate (%)':'credit_delinquency_rate',\n",
    "                        'recession_probability (%)':'recession_probability',\n",
    "                        'federal_funds_rate (%)':'federal_funds_rate',\n",
    "                        'manufacturers_new_orders_durable_goods (millions of dollars)':'manufacturers_new_orders_durable_goods',\n",
    "                        'manufacturer_new_orders_consumer_goods (millions of dollars)':'manufacturer_new_orders_consumer_goods',\n",
    "                        'total_vehicle_sales (millions of units)':'total_vehicle_sales',\n",
    "                        'new_one_family_houses_sold (thousands)':'new_one_family_houses_sold',\n",
    "                        'total_employment (thousands)':'total_employment',\n",
    "                         'consumer_credit (billions of dollars)':'consumer_credit'\n",
    "                             },\n",
    "                    ) ).copy()\n",
    "print('\\n%s\\n'%df.columns) \n",
    "\n",
    "print(\"\\n___ Descriptive stats of the probability of recession____\")\n",
    "print (df['recession_probability'].describe())\n",
    "\n",
    "print(\"\\n____Categorising probabilities of recession_____\")\n",
    "df['recession_probability'] = df['recession_probability']/100\n",
    "\n",
    "#####################################\n",
    "# ++ np.where ++ \n",
    "#          numpy.where(condition[, x, y])\n",
    "#          Return elements chosen from x or y depending on condition.\n",
    "#          https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html\n",
    "print(\"\\n____Some fun with where____\\n\")\n",
    "np.where([[True, False], [True, True]],\n",
    "         [[1, 2], [3, 4]],\n",
    "         [[9, 8], [7, 6]])\n",
    "\n",
    "df['recession']   = np.where(df['recession_probability']>.50,1, 0)\n",
    "print(df['recession'].value_counts())\n",
    "\n",
    "\n",
    "#####################################\n",
    "# ++ pandas.DataFrame.isna ++ \n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isna.html\n",
    "# Return a boolean same-sized object indicating if the values are NA\n",
    "print(\"\\n____Number of missing values by column____\")\n",
    "# print(df.isna().head())\n",
    "print(df.isna().sum(axis=0).sort_values(ascending=False).head(99))\n",
    "\n",
    "\n",
    "#####################################\n",
    "# ++ pandas.DataFrame.sum ++\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sum.html\n",
    "print(\"\\n____Number of missing values total: %d____\"%df.isna().sum(axis=0).sum(axis=0))\n",
    "\n",
    "\n",
    "print(\"\\n____Dropping missing values____\")\n",
    "df2 = df.dropna()\n",
    "df2 = df2.reset_index()            # https://stackoverflow.com/questions/40755680/how-to-reset-index-pandas-dataframe-after-dropna-pandas-dataframe\n",
    "df2 = df2.drop(['index'], axis=1)\n",
    "df2['year']  = df2['year'].astype('category')\n",
    "df2['month'] = df2['month'].astype('category')\n",
    "\n",
    "print(\"\\n______Data types____\")\n",
    "df2.info()\n",
    "\n",
    "df2['manufacturers_new_orders_durable_goods'] = df2['manufacturers_new_orders_durable_goods'].astype(float)\n",
    "df2['industrial_production_index'] = df2['industrial_production_index'].astype(float)\n",
    "df2['new_one_family_houses_sold'] = df2['new_one_family_houses_sold'].astype(float)\n",
    "\n",
    "num_df = df2.select_dtypes(['int', 'float'])\n",
    "cat_df = df2.select_dtypes(['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dropping the missing observations, we have 28 years of data spanning over 335 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = num_df.corr()\n",
    "corr = np.round(corr, 3)\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=0.6, center=0, annot= True,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation values\n",
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART A: Insights\n",
    "We use correlation analysis to find interesting insights in our data\n",
    "\n",
    "**1. How does consumers behaviours change with economic factors?**\n",
    "\n",
    "Consumer behaviours are measured using consumer credit and consumer opinion.\n",
    "\n",
    "1. Consumer credit is a personal debt taken on to purchase everyday goods and services. We see that consumer credit is positively associated (0.93) with total employment. That is, as more people join the workforce consumer credit increases.\n",
    "\n",
    "2. Also, as expected consumer credit (-0.6) is negatively associated with credit delinquency rate. In other words, a rise in deliquent rates leads to lower customer credit as people tend to be morre cautious with their spending.\n",
    "\n",
    "3. Interestingly, we find that as consumer credit increases the number of new housees sold decreases (-0.34). This is understandable as people need money/good credit score to buy houses so when they owe debt on purchasing basic amenities, they can't afford to pay for houses.\n",
    "\n",
    "4. We see that when people are optimistic about their finances and state of the economy they **tend** to increase their comfort by purchasing cars and houses. This is explained by the significant positive association between consumer opinion and total_vehicle_sales (0.434), new_one_family_houses_sold (0.545). On the other hand, if they perceive a downturn in the economy, as expected they become pessimistic (this is explained by the negative association when there is an increase in unemployment rate (-0.77) and recession (-0.493)).\n",
    "\n",
    "\n",
    "**2. How is the manufacturing sector affected by the probability of recession?**\n",
    "\n",
    "Variables used to measure the performance of the manufacturing industry are the number of new orders for durable goods, consumer goods and vehicle sales.\n",
    "\n",
    "Durable goods orders reflects new orders placed with domestic manufacturers for delivery of factory hard goods (durable goods) in the near term or future. Durable goods orders provide more insight into the supply chain than most indicators and can be especially useful in helping investors understand the earnings in industries, such as machinery, technology manufacturing, and transportation.\n",
    "\n",
    "Consumer goods A decreasing level of new orders is a worrying signal for the economy, as the level of retail sales act as a forward indicator for job growth at retail establishments and for companies manufacturing these products.\n",
    "\n",
    "1. We see that as more people join the workforce, there will be more orders for durable goods which is a good sign for the manufacturing industry. This is evident in the strong positive correlation (0.84) between total_employment and orders for manufacturers_new_orders_durable_goods.\n",
    "\n",
    "2. We also see that there is a strong positve association between orders for durable goods and consumer debt (0.86), vehicle sales (0.43).\n",
    "\n",
    "3. On the other hand, orders for durable goods is negatively associated with unemployment rate and credit delinquent rate. Also when there's a high probability of the economy going into recession, the orders for durable goods decreases. The changing pattern of demand in the consumer goods sector impacts companies operating in the retail.\n",
    "\n",
    "4. The effect on orders for consumer goods is similar to that of durable goods. However, we see that when there's an increase in the number of houses sold, the demand for consumer goods increases.\n",
    "\n",
    "A high durable goods number indicates an economy on the upswing while a low number indicates a downward trajectory.\n",
    "In sum, an increase in number of people gainfully employed, vehicle sales and consumer credit is a signal for the manufacturing industry that there will be more demand for durable and consumer goods.\n",
    "\n",
    "**3. How is the financial sector by government policies?**\n",
    "We use credit delinquent rate and federal funds rate to measure the performance of the financial sector.\n",
    "\n",
    "Federal funds rate refers to the interest rate that banks charge other banks for lending to them excess cash from their reserve balances on an overnight basis.\n",
    "\n",
    "1. We see that there is a strong negative association between credit delinquent rate and number of people joining the workforce, consumer credit, demand for durable and consumer goods, vehicle sales.\n",
    "\n",
    "2. As anticipated, an increase in credit delinquentrate leads to a corresponding increase in unemployment rate, federal funds rate and higher chances of recession.\n",
    "\n",
    "3. The effect is a bit different for federal funds rate as an increase in federal funds rate leads to an increases in house sales, credit delinquent rate and people are a bit optimistic about the state of the economy. However, it leads to a decrease in consumer credit and unemoployment rate. \n",
    "\n",
    "**4. How is the government affected by a downturn in the economy?**\n",
    "\n",
    "When unemployment rate increases:\n",
    "- People's faith in government (consumer opinion) decreases rapidly\n",
    "- The manufacturing bleeds as the demand for durable & consumer goods, vehicle & house sales decreases.\n",
    "\n",
    "When the likelihood of recession increases: \n",
    "- Credit delinquency rate increases\n",
    "- People become pessimistics, demand for consumer goods decreases as there is not sufficient money in circulation to purchase basic amenities.\n",
    "\n",
    "**5. How do we know when an economy is booming?**\n",
    "- When there's low unemployment rate, high consumer credit, manufacturing industry post profits, high vehicle and house sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALISATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALISATION\n",
    "sns.lineplot(x=\"year\", y=\"unemployment_rate\",\n",
    "             hue=\"recession\",\n",
    "             data=df)\n",
    "plt.title(\"Unemployement rate in the face of recession\")\n",
    "plt.ylabel(\"Unemployment rate (%)\")\n",
    "plt.xlabel(\"Year\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"unemployment_rate\", y=\"recession_probability\", data=df2,\n",
    "        ci=None, palette=\"muted\", height=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(df2[['year']], df2[['credit_delinquency_rate']], color='k', label =' credit_del_rate')\n",
    "plt.plot(df2[['year']], df2[['unemployment_rate']], color='g', label = 'unemp_rate')\n",
    "plt.plot(df2[['year']], df2[['federal_funds_rate']], color='r', label ='fed_funds_rate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "sns.boxplot(x=\"recession\", y=\"manufacturers_new_orders_durable_goods\", data = df2)\n",
    "plt.title(\"Manufacturers new orders for durable goods\")\n",
    "plt.ylabel(\"Number of orders\")\n",
    "plt.xlabel(\"Is there a recession?\")\n",
    "plt.xticks([0,1],['No','Yes'])\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "sns.boxplot(x=\"recession\", y=\"manufacturer_new_orders_consumer_goods\", data = df2)\n",
    "plt.title(\"Manufacturers new orders for consumer goods\")\n",
    "plt.ylabel(\"Number of orders\")\n",
    "plt.xlabel(\"Is there a recession?\")\n",
    "plt.xticks([0,1],['No','Yes'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.distplot(df2['total_vehicle_sales'])\n",
    "plt.title(\"Distribution plot of vehicle sales\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sns.distplot(df2['new_one_family_houses_sold'])\n",
    "plt.title(\"Distribution plot of new houses sales\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART B: MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the jargon of machine learning we are using supervised learning models for binary classification: we know the observed outcome (i.e., $y_t$ = $0$ or $1$) and want to assess how well we can predict these values using a variety of models. \n",
    "\n",
    "**Steps**\n",
    "\n",
    "- Our main variable of interest is $y_{t}$ which takes a value of $1$ if we observe recession in year $t$ in country (0 otherwise).\n",
    "\n",
    "- To do this, we classify recession probabilities into: 1 (recession if recession probability is > 0.5) and 0 otherwise.\n",
    "\n",
    "\n",
    "It is important to note that majority of observations are $y_t$ = 0 (no recession). In total, we end up with 335 observations, of which there are 17 recession periods (5% over the full sample). This infrequency poses a challenge to all of the methods considered.\n",
    "\n",
    "**Aim**\n",
    "\n",
    "- **Find the determinants of recession:** To do this, we will be  more interested in the sign/significance levels of individual parameters, rather than predictive accuracy. For the ease of interpretation, we use logistic regression to determine the determinants of recession.\n",
    "\n",
    "- **Forecast when the economy will be in a recession:** We are more interested in estimating the recession probabilities of the future (out-of-sample predictions) than in the past (in-sample predictions).\n",
    "\n",
    "**Models to be considered**\n",
    "- Logistic Regression\n",
    "- K nearest Neighbours (KNN)\n",
    "- Random Forest Classification (RF)\n",
    "- Support Vector Machine (SVM)\n",
    "\n",
    "Anticipating our main results, we find that the performance of ML methods heavily depends on whether we look at in-sample or out-of-sample predictions.\n",
    "\n",
    "\n",
    "**Model Accuracy**\n",
    "\n",
    "We use the logloss as a performance measure since we're interested in actual cases of recession (that is, $y_{t} = 1$) and our dataset is highly imbalanced. The logloss provides a steep penalty for predictions that are both wrong and confident (that is, it penalises our model when a high probability is assigned to the incorrect class).\n",
    "\n",
    "$$ logloss = \\frac{-1}{N} \\sum_{t=1}^{T} [y_{t} log (p_{t}) + (1 - p_{t})log (1- p_{t}) ] $$\n",
    "\n",
    "Where \n",
    "- Actual value (recession): y = {1 = yes, 0= no}\n",
    "- Prediction (Probability that y is 1): p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Determinants of Recession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = df2.drop(['observation_date','total_employment','recession','manufacturers_new_orders_durable_goods','manufacturer_new_orders_consumer_goods','recession_probability', 'year', 'month'],axis=1)\n",
    "y = df2[['recession']]\n",
    "\n",
    "X[X.columns] = scaler.fit_transform(X[X.columns])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.discrete import discrete_model\n",
    "logit_mod = discrete_model.Logit(y, X)\n",
    "print(logit_mod.fit().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "Recall that first we are interested in the sign/significance of individual parameters not predictive accuracy. To avoid multicollinearity, we dropped highly correlated variables such as:\n",
    "\n",
    "- Unemployment rate since we have total employment in the data\n",
    "- Manufacturing orders for durable and consumer goods since we have the industrial production index which measures measures the levels of production by the manufacturing sector, mining – including oil and gas field drilling services and electrical and gas utilities. It also measures capacity, an estimate of the production levels that could be sustainably maintained; and capacity utilization, the ratio of actual output to capacity.\n",
    "\n",
    "**Summary**\n",
    "- From the fitted model, the coefficient of **credit_delinquency_rate** says that holding other variables at a fixed value, we will see a 72% increase in the odds of the economy being in a recession for a one-percent increase in credit delinquency rate since $e^{0.5426} = 1.72$. We may also interpret in terms of probability, by saying all other variables held constant, the probability of an ecnonomy being in a recession given a 1% increase in credit delinquency rate is 63%. \n",
    "    \n",
    "    $$\\frac{exp^{0.5426}}{1 + exp^{0.5426}} = 0.63$$\n",
    "\n",
    "- Similarly, holding other variables constant, we will see a 104% increase in the odds of the economy being in a recession for a one-unit increase in **industrial production index rate** since $e^{0.7141} = 2.04$. Also, the probability of the economy being in recession given other variables are held constant is 67% for a unit-increase in industrial production index.\n",
    "     $$\\frac{exp^{0.7141}}{1 + exp^{0.7141}} = 0.67$$\n",
    "\n",
    "What if an interaction exists between some of the economic variables but we did not capture it in the simplified model above, will our model improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Effects in Regression\n",
    "\n",
    "An interaction effect exists when the effect of an independent variable on a dependent variable changes, depending on the value(s) of one or more other independent variables.\n",
    "\n",
    "When an interaction effect exists, the effect of one independent variable depends on the value(s) of one or more other independent variables.\n",
    "\n",
    "In our case, we are interested in the how consumer behaviour changes in when the economy is in a recession. Particulary, we will include an interaction effect bertween conusmer opinion and consumer credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['credit_and_opinion'] = X['consumer_credit']*X['consumer_opinion']\n",
    "logit_mod2 = discrete_model.Logit(y, X)\n",
    "print(logit_mod2.fit().summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "With an interaction term, our interpretation changes since the interaction term (credit_and_opinion) is significant (0.62). We see that the effect of consumer credit depends on the consumer opinion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recession Prediction\n",
    "\n",
    "Given that we do not separate the data into training and test sets in this case, the predictive accuracies from this exercise can be misleading.\n",
    "\n",
    "### 2.1 In-Sample Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_____________Logistic Regression________\n",
    "y = df2['recession']\n",
    "c_space = np.logspace(1,5,5)\n",
    "cgrid = {'C':c_space}\n",
    "logreg = GridSearchCV(LogisticRegression(solver = 'lbfgs'),cgrid, scoring = 'neg_log_loss', cv=3)\n",
    "logreg.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#____________ Classification Tree CART_________\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DecisionTreeClassifier().get_params()\n",
    "c_dt = {\n",
    "        'max_depth':[None, 3,4,5],\n",
    "        'min_samples_leaf':[1, 3,5],\n",
    "        'max_features':[None, 0.2, 0.4, 0.6]\n",
    "        }\n",
    "cp = DecisionTreeClassifier()\n",
    "cdt = GridSearchCV(cp, c_dt, cv=3, scoring = 'neg_log_loss')\n",
    "cdt.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_____________ EXTREME GRADIENT BOOST CLASSIFER_______\n",
    "import xgboost as xgb\n",
    "xc = {\n",
    "       'max_depth':[3, 5],\n",
    "       'n_estimators':[100,300]\n",
    "       }\n",
    "\n",
    "xcgrid= GridSearchCV(xgb.XGBClassifier(seed=123), \n",
    "                xc, cv=3, \n",
    "                scoring = 'neg_log_loss')\n",
    "xcgrid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#____________Random Forest Classifier_________\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "r_dt = {\n",
    "        'n_estimators':[100,200,300]\n",
    "        }\n",
    "rf = RandomForestClassifier()\n",
    "rft = GridSearchCV(rf, r_dt, cv=3, scoring = 'neg_log_loss')\n",
    "rft.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = pd.DataFrame({\n",
    "        'Model':[],\n",
    "        'logloss': [],\n",
    "        }) \n",
    "\n",
    "all_models = all_models.append({\n",
    "        'Model': 'Logistic Regression',\n",
    "        'logloss': np.abs(logreg.best_score_)\n",
    "        }, ignore_index = True)\n",
    "\n",
    "\n",
    "all_models = all_models.append({\n",
    "        'Model': 'CART',\n",
    "        'logloss': np.abs(cdt.best_score_)\n",
    "        }, ignore_index = True)\n",
    "\n",
    "all_models = all_models.append({\n",
    "        'Model': 'Extreme Gradient Boosting',\n",
    "        'logloss': np.abs(xcgrid.best_score_)\n",
    "        }, ignore_index = True)\n",
    "\n",
    "all_models = all_models.append({\n",
    "        'Model': 'Random Forest',\n",
    "        'logloss': np.abs(rft.best_score_)\n",
    "        }, ignore_index = True)\n",
    "\n",
    "all_models = all_models.sort_values(by=['logloss'])\n",
    "all_models = all_models.reset_index()\n",
    "all_models.drop('index', axis=1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CONCLUSION\n",
    "A perfect classifier would have a Log Loss of precisely zero. Less ideal classifiers have progressively larger values of Log Loss.\n",
    " \n",
    "Log Loss heavily penalises classifiers that are confident about an incorrect classification. For example, if for a particular observation, the classifier assigns a very small probability to the correct class then the corresponding contribution to the Log Loss will be very large indeed. \n",
    "\n",
    "It is evident Logistic Regression has the least logloss (0.104) followed by Extreme Gradient Boosting (XGB) whereas decision trees performs worst.\n",
    "\n",
    "Here we showed that ML methods can be useful for different prediction problems, **but may not always** outperform more traditional approaches (such as Logistic regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all\n",
    "all_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Areas of improvement\n",
    "\n",
    "1. Hyperparameter tuning\n",
    "2. Get more data\n",
    "3. More interesting visualisations\n",
    "4. Perforn out-of-sample predictions\n",
    "\n",
    "### Performing out-of-sample predictions\n",
    "\n",
    "We do not show this as we know these chances of overfitting is high since the models could not even perform perfectly on the training data.\n",
    "\n",
    "The typical approach is K-fold cross-validation where one splits the dataset into K equally sized blocks, and for each block trains a model using data from all other blocks only. One obvious issue with this approach is that it ignores that we are dealing with time-series predictions here: we do not want to train a model for predicting a recession in the past. Therefore, we mainly focus on another validation approach, which again splits the data into K equally-sized blocks, but trains each model using information on previous blocks only. \n",
    "\n",
    "This is called Time Series Split in sklearn (<a href=\"https://scikitlearn.org/stable/modules/cross_validation.html\">Read More</a>)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
